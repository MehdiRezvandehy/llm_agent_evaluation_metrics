# llm_agent_evaluation_metrics_bias_rubrics

This notebook introduces a framework for evaluating large language models (LLMs) and AI agents, highlighting key evaluation metrics and the issue of positional bias. It explains the rubric-based evaluation method, a scoring approach often used in educational settings. The notebook also compares model performance with and without chain-of-thought reasoning and discusses strategies for mitigating LLM bias. Finally, it presents several in-depth examples to further explore the impact of positional bias.
